{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "QkclVdIHP4_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pav-rTkz9XiS"
      },
      "outputs": [],
      "source": [
        "!pip install \"cassio>=0.1.0\" llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Document,\n",
        "    StorageContext,\n",
        ")\n",
        "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
        "from llama_index.vector_stores import CassandraVectorStore\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "1HSMxgkn9fFF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "database connection parameters and secrets"
      ],
      "metadata": {
        "id": "qGQFxcxi93J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IS_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# Your database's Secure Connect Bundle zip file is needed:\n",
        "if IS_COLAB:\n",
        "    print('Please upload your Secure Connect Bundle zipfile: ')\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "        ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "        )\n",
        "else:\n",
        "    # you are running a local-jupyter notebook:\n",
        "    ASTRA_DB_SECURE_BUNDLE_PATH = input(\"Please provide the full path to your Secure Connect Bundle zipfile: \")"
      ],
      "metadata": {
        "id": "cWMNycp0936F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "4b372498-1e7a-4e39-9e70-b6d5bffca6d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Secure Connect Bundle zipfile: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6271a5c0-d675-4823-9d12-56b7547d5749\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6271a5c0-d675-4823-9d12-56b7547d5749\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving secure-connect-voldemort-vector.zip to secure-connect-voldemort-vector.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Please provide your Database Token ('AstraCS:...' string): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwT2OC5iEiRB",
        "outputId": "a2c311a8-5a9a-4fa2-c8c3-625191a96fc3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Database Token ('AstraCS:...' string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_KEYSPACE = input(\"Please provide the Keyspace name for your Database: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT8-veyiEh7z",
        "outputId": "98382220-0be3-4e19-ed07-7b4cc63e16f0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide the Keyspace name for your Database: llamaindex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "#Establish Connectivity\n",
        "cluster = Cluster(\n",
        "    cloud={\n",
        "        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
        "    },\n",
        "    auth_provider=PlainTextAuthProvider(\n",
        "        \"token\",\n",
        "        ASTRA_DB_APPLICATION_TOKEN,\n",
        "    ),\n",
        ")\n",
        "\n",
        "session = cluster.connect()\n",
        "keyspace = ASTRA_DB_KEYSPACE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2nOTJng-hou",
        "outputId": "c3538a53-7e45-45fe-879b-ba98a21bc8c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for b2d68f45-7d3e-4936-a50c-b0742b34b3f5-us-east-2.db.astra.datastax.com:29042:2c823767-4e6e-4c32-ba6f-fed79fd419f1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for b2d68f45-7d3e-4936-a50c-b0742b34b3f5-us-east-2.db.astra.datastax.com:29042:2c823767-4e6e-4c32-ba6f-fed79fd419f1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(135010588228096) b2d68f45-7d3e-4936-a50c-b0742b34b3f5-us-east-2.db.astra.datastax.com:29042:2c823767-4e6e-4c32-ba6f-fed79fd419f1> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for b2d68f45-7d3e-4936-a50c-b0742b34b3f5-us-east-2.db.astra.datastax.com:29042:2c823767-4e6e-4c32-ba6f-fed79fd419f1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FdGc3fAFvME",
        "outputId": "be119e74-0aca-4159-f8f6-4c9e11af3eb5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "my0xLEpFF75A"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating and populating the Vector Store"
      ],
      "metadata": {
        "id": "GnoWrPaMHrzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"/content/paul_graham\").load_data()\n",
        "print(f\"Total documents: {len(documents)}\")\n",
        "print(f\"First document, id: {documents[0].doc_id}\")\n",
        "print(f\"First document, hash: {documents[0].hash}\")\n",
        "print(\n",
        "    f\"First document, text ({len(documents[0].text)} characters):\\n{'='*20}\\n{documents[0].text[:360]} ...\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcaOK568GA0C",
        "outputId": "7039cc2d-8cb7-4d06-fb3c-af25a8c68144"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 1\n",
            "First document, id: cc1dab80-e247-4407-8e15-7cf878f592c1\n",
            "First document, hash: 2e2d9629223c077019a6dde689049344ff2293d6c52372871420119ec049f25c\n",
            "First document, text (75014 characters):\n",
            "====================\n",
            "\n",
            "\n",
            "What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined ma ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Cassandra Vector Store\n",
        "\n",
        "Creation of the vector store entails creation of the underlying database table if it does not exist yet:"
      ],
      "metadata": {
        "id": "sEYdZ5Y2IcLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cassandra_store = CassandraVectorStore(\n",
        "    session=session,\n",
        "    keyspace=keyspace,\n",
        "    table=\"cassandra_vector_table_1\",\n",
        "    embedding_dimension=1536,\n",
        ")"
      ],
      "metadata": {
        "id": "u_bmPWtiIhIl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now wrap this store into an index LlamaIndex abstraction for later querying:\n"
      ],
      "metadata": {
        "id": "j7iH2eZIJV5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storage_context = StorageContext.from_defaults(vector_store=cassandra_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
      ],
      "metadata": {
        "id": "KqDuhdCoJXEo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the above from_documents call does several things at once: it splits the input documents into chunks of manageable size (“nodes”), computes embedding vectors for each node, and stores them all in the Cassandra Vector Store.\n",
        "\n",
        "# **Querying the store**\n",
        "\n",
        "Basic querying"
      ],
      "metadata": {
        "id": "whmdFkD7JzDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Why did the author choose to work on AI?\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTLMXBMqKEJs",
        "outputId": "050008d3-f2d0-4f3d-dc86-d71063c494ac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author chose to work on AI because they were inspired by a novel called \"The Moon is a Harsh Mistress\" by Heinlein, which featured an intelligent computer called Mike. Additionally, they were influenced by a PBS documentary that showed Terry Winograd using SHRDLU, a program that they believed could be improved by teaching it more words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MMR-based queries**\n",
        "\n",
        "The MMR (maximal marginal relevance) method is designed to fetch text chunks from the store that are at the same time relevant to the query but as different as possible from each other, with the goal of providing a broader context to the building of the final answer:"
      ],
      "metadata": {
        "id": "7YMCWIWZKdr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(vector_store_query_mode=\"mmr\")\n",
        "response = query_engine.query(\"Why did the author choose to work on AI?\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWh5aNH5Ki00",
        "outputId": "12040260-43a8-4c84-9f44-148030017049"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author chose to work on AI because they believed that it was a field that had the potential to climb the lower slopes of intelligence. They were excited about the possibilities of teaching programs like SHRDLU to understand natural language and expand their concept of a program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HyDE Query Transform**\n",
        "\n",
        "HyDE stands for Hypothetical Document Embeddings, is a technique used in semantic search to find documents based on similarities in semantic embedding. It’s a zero-shot learning technique, meaning it can make predictions about data it has not been trained on.\n",
        "\n",
        "In the context of search, HyDE works by generating a hypothetical answer to a query using a language model. This hypothetical answer is then embedded into a vector space, similar to how real documents are embedded. When a search query comes in, similar real documents are retrieved based on vector similarity to the hypothetical document. This allows for a more precise and relevant retrieval of documents, even when the exact terms used in the search query may not be present in the documents.\n",
        "\n",
        "The aim of HyDE is to improve the quality of search results by focusing on the underlying intent of the search query, rather than just the exact words used. This makes it particularly useful for tasks like question-answering, where the goal is to find the most relevant information to answer a user’s question, rather than just finding documents that contain the exact words used in the question"
      ],
      "metadata": {
        "id": "-mfMdLxAXNa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, we query without transformation: The same query string is used for embedding lookup and also summarization."
      ],
      "metadata": {
        "id": "SIfS2iVqX8bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = \"what did paul graham do after going to RISD\""
      ],
      "metadata": {
        "id": "gT6Xfz_KXoHs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(query_str)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lesL9cU4YAz5",
        "outputId": "306aa2e0-16a8-4cff-af58-0f1e6e9b155f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>After going to RISD, Paul Graham did freelance work for a group that did projects for customers.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup."
      ],
      "metadata": {
        "id": "2toOSuhJYcnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
        "response = hyde_query_engine.query(query_str)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "x0UGyeJaYe97",
        "outputId": "172c710d-d260-47c0-ff7c-de6fc0208b2d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>After going to RISD, Paul Graham dropped out and moved to New York. He then decided to write another book on Lisp and became a studio assistant for Idelle Weber, a painter. Additionally, he started a company with Robert Morris to put art galleries online, but the idea did not succeed.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, HyDE improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output."
      ],
      "metadata": {
        "id": "ByZSVFPiZNId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_bundle = hyde(query_str)\n",
        "hyde_doc = query_bundle.embedding_strs[0]\n",
        "display(Markdown(f\"<b>{hyde_doc}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "T-GN9knbZPer",
        "outputId": "e7faa697-9103-4bf1-e4f0-4a93143c943f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>After attending the Rhode Island School of Design (RISD), Paul Graham embarked on a remarkable journey that would shape his future as a successful entrepreneur and influential figure in the tech industry. Armed with a degree in painting, Graham initially pursued his passion for art, immersing himself in the vibrant art scene and exhibiting his work in various galleries.\n\nHowever, Graham's insatiable curiosity and innate problem-solving abilities led him to explore the world of computer programming. Recognizing the immense potential of technology, he delved into coding and quickly became enamored with its limitless possibilities. This newfound passion prompted Graham to shift his focus and embark on a new path.\n\nIn 1995, Graham co-founded Viaweb, an early e-commerce platform that allowed users to build their online stores. This groundbreaking venture not only showcased Graham's entrepreneurial spirit but also demonstrated his ability to identify emerging trends and capitalize on them. Viaweb's success caught the attention of Yahoo, which acquired the company in 1998, solidifying Graham's reputation as a tech visionary.\n\nFollowing the acquisition, Graham continued to make significant contributions to the tech industry. In 2001, he co-founded Y Combinator, a startup accelerator that provides funding and mentorship to early-stage companies. Y Combinator quickly gained recognition as one of the most prestigious and influential startup incubators, nurturing the growth of countless successful companies, including Dropbox, Airbnb, and Reddit.\n\nGraham's impact extended beyond his role at Y Combinator. He became a prolific writer, sharing his insights and experiences through thought-provoking essays. His writings, which covered a wide range of topics including entrepreneurship, technology, and philosophy, garnered a large following and solidified his status as a respected intellectual.\n\nIn addition to his entrepreneurial endeavors and writing, Graham also dedicated himself to philanthropy. He and his wife, Jessica Livingston, established the Paul Graham Foundation, which supports various charitable causes, including education and poverty alleviation.\n\nIn summary, after attending RISD, Paul Graham transitioned from the world of art to the realm of technology. Through his ventures, such as Viaweb and Y Combinator, he demonstrated his entrepreneurial prowess and ability to identify and nurture promising startups. Graham's writings and philanthropic efforts further solidified his influence and impact, making him a prominent figure in both the tech industry and the intellectual community.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Connecting to an existing store**\n",
        "\n",
        "Since this store is backed by Cassandra, it is persistent by definition. So, if you want to connect to a store that was created and populated previously, here is how:"
      ],
      "metadata": {
        "id": "6GDgQNpXKxtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_store_instance = CassandraVectorStore(\n",
        "    session=session,\n",
        "    keyspace=keyspace,\n",
        "    table=\"cassandra_vector_table_1\",\n",
        "    embedding_dimension=1536,\n",
        ")\n",
        "\n",
        "# Create index (from preexisting stored vectors)\n",
        "new_index_instance = VectorStoreIndex.from_vector_store(vector_store=new_store_instance)\n",
        "\n",
        "# now you can do querying, etc:\n",
        "query_engine = new_index_instance.as_query_engine(similarity_top_k=5)\n",
        "response = query_engine.query(\"What did the author study prior to working on AI?\")\n",
        "\n",
        "print(response.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whk_Gc-_K2ii",
        "outputId": "5ac0fdae-bd41-4353-d30f-d445e483a125"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior to working on AI, the author studied painting and drawing at the Accademia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing documents from the index**\n",
        "\n",
        "First get an explicit list of pieces of a document, or “nodes”, from a Retriever spawned from the index:"
      ],
      "metadata": {
        "id": "ehWLoPUpLhFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = new_index_instance.as_retriever(\n",
        "    vector_store_query_mode=\"mmr\",\n",
        "    similarity_top_k=3,\n",
        "    vector_store_kwargs={\"mmr_prefetch_factor\": 4},\n",
        ")\n",
        "nodes_with_scores = retriever.retrieve(\n",
        "    \"What did the author study prior to working on AI?\"\n",
        ")\n",
        "\n",
        "print(f\"Found {len(nodes_with_scores)} nodes.\")\n",
        "for idx, node_with_score in enumerate(nodes_with_scores):\n",
        "    print(f\"    [{idx}] score = {node_with_score.score}\")\n",
        "    print(f\"        id    = {node_with_score.node.node_id}\")\n",
        "    print(f\"        text  = {node_with_score.node.text[:90]} ...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V2BmcscLlal",
        "outputId": "86068711-8453-4e83-c6e0-00b0e0d6d57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 nodes.\n",
            "    [0] score = 0.4293121408693589\n",
            "        id    = 57c1b2d3-0ef0-4c93-8707-24be53f3045a\n",
            "        text  = What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside o ...\n",
            "    [1] score = 0.002232291606439618\n",
            "        id    = d0509622-bf5d-482b-a70c-f357effc31a7\n",
            "        text  = Now all I had to do was learn Italian.\n",
            "\n",
            "Only stranieri (foreigners) had to take this entra ...\n",
            "    [2] score = 0.02296870053065997\n",
            "        id    = f6abf4c1-a1f2-428e-862a-407409edd3d8\n",
            "        text  = All you had to do was teach SHRDLU more words.\n",
            "\n",
            "There weren't any classes in AI at Cornell ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print nodes ref_doc_id they all should have same as we inserted only one record."
      ],
      "metadata": {
        "id": "48G5-2J3MQAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nodes' ref_doc_id:\")\n",
        "print(\"\\n\".join([nws.node.ref_doc_id for nws in nodes_with_scores]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Ng9jR4Mi65",
        "outputId": "bb80334e-9eea-4028-f84c-ba33d8989b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes' ref_doc_id:\n",
            "5364256d-d5cc-4a62-acd2-73fe7aa52784\n",
            "5364256d-d5cc-4a62-acd2-73fe7aa52784\n",
            "5364256d-d5cc-4a62-acd2-73fe7aa52784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to remove the text file you uploaded:"
      ],
      "metadata": {
        "id": "_VuRf718MrMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_store_instance.delete(nodes_with_scores[0].node.ref_doc_id)"
      ],
      "metadata": {
        "id": "X0w5PGh0MsuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the very same query and check the results now. You should see no results being found:"
      ],
      "metadata": {
        "id": "l-aUDDXQMzbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_with_scores = retriever.retrieve(\n",
        "    \"What did the author study prior to working on AI?\"\n",
        ")\n",
        "\n",
        "print(f\"Found {len(nodes_with_scores)} nodes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWCbRlHyM0pQ",
        "outputId": "c9e3c20b-76d7-4686-e78d-b1491f79994f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 nodes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata filtering**\n",
        "\n",
        "The Cassandra vector store support metadata filtering in the form of exact-match key=value pairs at query time.\n",
        "\n",
        "In this demo, a single source document is loaded (the paul_graham_essay.txt text file). we will attach some custom metadata to the document to illustrate how we can can restrict queries with conditions on the metadata attached to the documents."
      ],
      "metadata": {
        "id": "xmnyl1n6NkXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "md_storage_context = StorageContext.from_defaults(\n",
        "    vector_store=CassandraVectorStore(\n",
        "        session=session,\n",
        "        keyspace=keyspace,\n",
        "        table=\"cassandra_vector_table_2_md\",\n",
        "        embedding_dimension=1536,\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "def my_file_metadata(file_name: str):\n",
        "    \"\"\"Depending on the input file name, associate a different metadata.\"\"\"\n",
        "    if \"essay\" in file_name:\n",
        "        source_type = \"essay\"\n",
        "    elif \"dinosaur\" in file_name:\n",
        "        # this (unfortunately) will not happen in this demo\n",
        "        source_type = \"dinos\"\n",
        "    else:\n",
        "        source_type = \"other\"\n",
        "    return {\"source_type\": source_type}\n",
        "\n",
        "\n",
        "# Load documents and build index\n",
        "md_documents = SimpleDirectoryReader(\n",
        "    \"/content/paul_graham\", file_metadata=my_file_metadata\n",
        ").load_data()\n",
        "md_index = VectorStoreIndex.from_documents(\n",
        "    md_documents, storage_context=md_storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "bbAjpLx1N4d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can now add filtering to your query engine:"
      ],
      "metadata": {
        "id": "HWFXjbFkPAqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
        "\n",
        "md_query_engine = md_index.as_query_engine(\n",
        "    filters=MetadataFilters(\n",
        "        filters=[ExactMatchFilter(key=\"source_type\", value=\"essay\")]\n",
        "    )\n",
        ")\n",
        "md_response = md_query_engine.query(\"How long it took the author to write his thesis?\")\n",
        "print(md_response.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBx3mqcPCJJ",
        "outputId": "2890bc20-7887-4452-cdd5-f44ec6d53126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty Response\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test that the filtering is at play, try to change it to use only \"dinos\" documents… there will be no answer"
      ],
      "metadata": {
        "id": "yJRjgB0UPiUT"
      }
    }
  ]
}